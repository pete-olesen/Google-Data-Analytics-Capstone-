---
title: "Bellabeat Capstone Project"
author: "Pete Olesen"
date: '2022-04-30'
personal website: 'www.peterolesen.com'
email address: 'olesenp2002@gmail.com'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = TRUE, echo = TRUE)
```

## About me:

My name is **Peter Olesen** and this is my capstone project for the **Google Data Analytics Certification Course** I am taking on Coursera.

I selected Case Study 2: Bellabeat - How can a wellness technology company play it smart?

I chose R Studio as the tool to use for this analysis for a couple of reasons. I wanted to grow my skill set. I have extensive experience with spreadsheets and could have used either Excel or Google Sheets to perform the data processing and analysis. I also wanted to apply what I learned about R as well as publish the analysis via html (knitting) to my personal website.

This is my first foray into using R.

## The Business Scenario:
This is in the Bellabeat case study description, but I have included it here for those who may not be familiar with the details.

I am a junior analyst working for the marketing analyst team at Bellabeat, a high-tech manufacturer of health-focused products for women.  Bellabeat management wants to become a larger player in the global smart device market. The co-founder and Chief Creative Officer(CCO), Urska Srsen,  believes that analyzing smart device fitness data could unlock new growth opportunities for the company.

Bellabeat currently has five products:

**Bellabeat app:** An application that provides users with health data related to activity, sleep, stress, menstrual cycle and mindfulness. This data can help users better understand their current habits and make healthy decisions.

**Leaf:** Bellabeat's classic wellness tracker that can be worn as a bracelet, necklace or clip. This device connects to the Bellbeat app.

**Time:** A wellness smart watch that provides insights to daily wellness.

**Spring:** A water bottle that tracks daily water intake.

**Bellabeat membership:** A subsription model that gives members 24/7 access to fully personalized guidance.

## Following the process defined in the coursework:

**Ask**

**Prepare**

**Process**

**Analyze**

**Share**

**Act**

## Ask:

The business task identified by the CCO is to analyze smart device usage data in order to determine how consumers use non-Bellabeat smart devices. Specific questions to answer are:

1. What are some trends in smart device usage?
2. How could these trends apply to Bellabeat customers?
3. How could these trends influence Bellabeat's marketing strategy?

## Prepare:

Srsen encourages me to use public data that explore smart device users daily habits. She points me to a specific dataset of Fitbit user data from thirty Fitbit users.  That data can be found here: [Fitbit Fitness Tracker Data](https://www.kaggle.com/datasets/arashnic/fitbit)

This data set includes minute level output for physical activity, heart rate to explore user's habits. Srsen has said that there are some limitations and asks me to consider another data to address limitations in this data set.

First things first, let's take a look at the data and any meta data to get a better understanding of what we are working with. 

I downloaded this data set from Kaggle and it consists of 18 csv files. I read the metadata section on Kaggle and it is licensed under CCO: Public domain. It also says the data collection methodology is pre-processed. I could not find a specific data dictionary, but did follow this link from Kaggle under [Provenance: Sources](https://zenodo.org/record/53894#.YmgbKtrMJPZ)    

Based on this link, the dataset was generated by a survey by Amazon Mechanical Turk between 3/12/2016 and 5/12/2016. Thirty Fitbit users consented to the use of their personal Fitbit usage data, including minute level output for physical activity, heart rate and sleep monitoring. Individual reports can be parsed by export session ID or by time stamp. Variation between output represents different types of Fitbit trackers and individual tracking behaviors and preferences.

I have saved all 18 files in the SourceData folder.

**Comments**

Based on this review, I am concluding that the data ROCC's, that is, it is reliable, original, current and cited. I still need to go through the data cleaning step but at this moment, I think it meets these criterion.

However, I do have some concerns about some of the aspects of this data set.

1. The limited number of users. This data set has 30 users. This is not a lot of users and may introduce significant bias in the analysis

2. The limited number of days of coverage, approximately 60 days in the spring of 2016.  Many people living in colder climates are anxious to get outside and become more fit in the spring.  This may not give a total picture of user's activities throughout the year.

3. There doesn't seem to be a gender identifier in the data. This is potentially significant to Bellabeat as it's customer base is currently geared heavily towards women. Moreover, the data does not appear to have demographic data.

My plan is to complete the analysis with this existing data set and report back to the stakeholders and I am calling this phase 1.  My analysis will identify the limitations as well as the specific recommendations. Likely, I will recommend a phase 2 that has a larger and more comprehensive data set if one is available.


## Process:

The first thing I want to do is to explore the data.  As mentioned earlier, there are 18 csv files in this data set. There are multiple tables, some at the daily level, others down to the hour and second. In keeping with the goals of project, I think the daily activity is the most relevant.  Our goal is to gain some insight(s) into Fitbit user activities to aid in our marketing efforts. The first cut should be at the daily level. If need be, we can go to the minute and second level.

I am using R studio and the tidyverse set of libraries for this analysis. I am using this because it is a full featured IDE and it has the capability to create markdown files that can share both the entire analysis with others and they can replicate the steps and run the associated code.

There are five potential daily files to explore:

**dailyActivity_merged**

**dailyCalories_merged**

**dailyIntensities_merged**

**dailySteps_merged**

**sleepDay_merged**

Let's load some libraries and get cracking..

``` {r echo=TRUE}
library(tidyverse)
library(janitor)
library(skimr)
library(here)
library(lubridate)
library(scales)
library(ggplot2)
library(gtsummary)
```
I want to explore each of the five the daily tables to see the data that is in each.
First let's load each of them to a data frame

``` {r echo=TRUE}
dailyActivity_merged_df <- read.csv(here("SourceData","dailyActivity_merged.csv"))
dailyCalories_merged_df <- read.csv(here("SourceData","dailyCalories_merged_01.csv"))
dailyIntensities_merged_df <- read.csv(here("SourceData","dailyIntensities_merged.csv"))
dailySteps_merged_df <- read.csv(here("SourceData","dailySteps_merged.csv"))
sleepDay_merged_df <- read.csv(here("SourceData","sleepDay_merged.csv"))
```

Let's use str to get some idea about the data structure of each daily file. Starting with dailyActivity_merged.

``` {r echo=TRUE}
str(dailyActivity_merged_df)
```
This file has 940 observations. Based on the field names, it looks like the variables are tracking ID, activity date, steps, distance, minutes and calories consumed.

Now the same exercise for each of the remaining files: dailyCalories_merged, dailyIntensities_merged, dailySteps_merged, and sleepDay_Merged.

``` {r echo=TRUE}
str(dailyCalories_merged_df)
str(dailyIntensities_merged_df)
str(dailySteps_merged_df)
str(sleepDay_merged_df)

```

It looks like three of the four remaining tables have 940 observations as well and are appear to be redundant with dailyActivity_merged.  For example, daily_Calories has ID, Date and Calories. When I compare calories for ID 1503960366 on 4/12/2016 between daily_Activity_merged and daily_Calories_merged, I get the same result, specifically 1,985 calories.  I did the same for SedentaryMinutes for this same ID and date between daily_Activity_merged and daily_Intensities_merged. Both were 728 minutes.

So, based on this, I am assuming that the dailyActivity_merged is a combined table and I will use this for the analysis.  The sleepDay_merged table does not have 940 observations, but 413 observations. It appears that for whatever reason, the sleep data is incomplete for each ID and date. 

I am proceeding with using dailyActivity_merged as the basis of this analysis. I believe it combines the majority of the data into one data frame. Now to clean..

## Cleaning data

Having settled on using dailyActivity_merged for the analysis, I want to make sure that we have a complete data set, that is to what extent do we have complete data for each ID and date?

Based on the description of the data, there were approximately 30 users (ID's) in the dataset. We should see this number of distinct ID's in the dataframe.

``` {r echo=TRUE}
unique_IDs <- length(unique(dailyActivity_merged_df$Id))
print(unique_IDs)
```

The answer was in the range I was expecting, specifically it returns 33 unique users (Ids).

I think that it warrants making sure we have a complete data set, or at least understand what items are missing. That is, the instances where each observation does not have a value in every field. To do this, I will use the complete.cases function to identify observations that are not complete. I will create two data frames, one for complete cases, the other for partial cases...

``` {r echo=TRUE}
complete_dailyActivity_merged_df <- dailyActivity_merged_df[complete.cases(dailyActivity_merged_df),]
partial_dailyActivity_merged_df <- dailyActivity_merged_df[!complete.cases(dailyActivity_merged_df),]
head(complete_dailyActivity_merged_df)
head(partial_dailyActivity_merged_df)
```

Since the complete cases returned 940 observations and the partial returned 0, I have concluded that the data set is complete. I was not expecting that, as I thought I would have to deal with missing values. 

The last thing I want to do is to change the activity date to a date field, and add a day of the week field. I have a suspicion that the physical activity will be higher on certain days of the week. This could be significant in the analysis and the marketing implications.  Currently, the date is in character format.

I have added three fields to the data set using Lubridate. Specifically ActivityDateNew (converting the activity date from CHR to a date field), ActivityDate_DOW_numb (the day of the week starting with Sunday), ActivityDate_DOW_Desc (the three character description of the activity date). I plan on using these fields in the analyze phase. 
``` {r echo=TRUE}
complete_dailyActivity_merged_df  <- complete_dailyActivity_merged_df %>% 
    mutate(ActivityDateNew = mdy(ActivityDate),
           ActivityDate_DOW_numb = wday(ActivityDateNew),
           ActivityDate_DOW_Desc = wday(ActivityDateNew, label = TRUE))
        head(complete_dailyActivity_merged_df)
```

The last step in cleaning, run clean_names against the data frame to get the field names into snake case which is recommended by the tidyverse style guide.

``` {r echo=TRUE}
complete_dailyActivity_merged_df  <- complete_dailyActivity_merged_df %>% 
    clean_names()
    View(complete_dailyActivity_merged_df)
```


## Analyze:

Now that the data is cleaned and prepared, we can get to the analysis.

As I indicated earlier, I am focused on the daily_Activity_merged table as it has the most complete data. I am excluding the sleepDay_merged as it has only 413 observations, and the activity data has 940 observations. This is a large drop off in records (>50%).  I suspect this is due to people not wearing their Fitbit devices when then sleep.

In any case, there seems to be four basic data elements per Id (user). Specifically, the number of steps, distance, duration (minutes) and calories.  Two of the four categories, distance and duration are further broken down into levels of intensity, very active, moderately active, lightly active and sedentary.

Let's take a look at the duration of the activity by intensity. I want to get a better sense of how many minutes users are spending at each intensity level. The goal here is to better understand the users in the sample. Since the cleaned data frame is in the wide format, I think it would graph better if it was in the long format. Essentially, I want to "pivot" the duration data.  Here is the code..

``` {r echo=TRUE}
activity_duration_long  <- complete_dailyActivity_merged_df %>% 
                    pivot_longer(c(lightly_active_minutes,fairly_active_minutes,very_active_minutes,                                                    sedentary_minutes),
                                  names_to ="activity_type",
                                  values_to = "activity_minutes") %>% 
                    select(c(id,activity_date_new,activity_date_dow_numb,activity_date_dow_desc,activity_type,activity_minutes)) 
                   
          

```

Let's take a look at the pivoted data frame with activity duration...


``` {r echo=TRUE}
glimpse(activity_duration_long)
head(activity_duration_long)
```

The data frame has captured the activity minutes by type. The original source data was 940 observations and this data frame is 3,760 observations which makes sense to me as it is 4 times the original data. There are four activity types, lightly_active, fairly_active, very_active, sedentary.

Let's make a bar chart by activity type to get a sense of the average minutes per day for each activity type

``` {r echo=TRUE}
 
activity_duration_long %>%
  group_by(activity_type) %>%
  summarise(avg_daily_minutes = mean(activity_minutes)) %>%
  arrange(avg_daily_minutes)  %>% 
  ggplot(aes(x = reorder(activity_type, avg_daily_minutes) ,y = avg_daily_minutes,digits , fill="blue")) +
  geom_bar(stat = "identity", fill="steelblue")+
  geom_text(aes(label = avg_daily_minutes), nudge_y = 40)+
  #scale_y_continuous(labels = label_number(accuracy = 0.1)+
  theme(legend.position = "none",axis.title.x = element_blank(),axis.title.y = element_blank())+
  labs(title = "Average Daily Minutes by Activity Type",
              subtitle = "All activity types",
              caption = "Data source: Fitbit data set")
```
As you can see from this chart, the average user spends most of their day (991 minutes or 16.5 hours a day) sedentary. The total active minutes is 228 minutes or 3.8 hours. Sorry about the decimals on the columns. Despite my best efforts, I was not able to get the round function to work here.

To see distribution more clearly, let's look at only categories with active minutes (exclude sedentary).

``` {r echo=TRUE}

activity_duration_long_active  <- complete_dailyActivity_merged_df %>% 
                    pivot_longer(c(lightly_active_minutes,fairly_active_minutes,very_active_minutes),
                                  names_to ="activity_type",
                                  values_to = "activity_minutes") %>% 
                    select(c(id,activity_date_new,activity_date_dow_numb,activity_date_dow_desc,activity_type,activity_minutes)) 
        
 
activity_duration_long_active %>%
  group_by(activity_type) %>%
  summarise(avg_daily_minutes = mean(activity_minutes)) %>%
  arrange(avg_daily_minutes)  %>% 
  ggplot(aes(x = reorder(activity_type, avg_daily_minutes) ,y = avg_daily_minutes,digits , fill="blue")) +
  geom_bar(stat = "identity", fill="steelblue")+
  geom_text(aes(label = avg_daily_minutes), nudge_y = 20)+
  #scale_y_continuous(labels = label_number(accuracy = 0.1)+
  theme(legend.position = "none",axis.title.x = element_blank(),axis.title.y = element_blank())+
  labs(title = "Average Daily Minutes by Activity Type",
              subtitle = "Non-Sedentary activity types",
              caption = "Data source: Fitbit data set")
```

Most of activity falls into lightly active at 192 minutes, and next is very active at 21 minutes.  Fairly active is averaging ~14 minutes per day.

Based on this, it looks like this sample of users is fairly active with a total of 45 minutes per day of fairly active and very active minutes. This makes sense as they are Fitbit users and likely fitness minded, but does validate they are actively using these devices. 

I would like to understand the distribution of the active minutes by day of the week. The goal would be to a better understanding of when people are active.  Are they equally active every day of the week, or are they active primarily on the weekends? This could be useful in Bellabeat's marketing efforts. Are people "weekend warriors" that work out heavily on weekends, or is fitness more of a daily lifestyle activity?

``` {r echo=TRUE}
activity_duration_long_active %>%
  group_by(activity_type, activity_date_dow_numb, activity_date_dow_desc) %>%
  summarise(avg_daily_minutes = mean(activity_minutes)) %>% 
  ggplot(aes(x = activity_date_dow_desc ,y = avg_daily_minutes,digits , group = activity_type, color =              activity_type))+
  geom_line()+
  # geom_text(aes(label = avg_daily_minutes), nudge_y = 20)+
  theme(legend.position = "bottom",axis.title.x = element_blank(),axis.title.y = element_blank())+
  labs(title = "Average Daily Minutes by Activity Type by Day of the Week",
              subtitle = "Non-Sedentary activity types",
              caption = "Data source: Fitbit data set")


```

Based on this plot, it looks like there is consistency by activity type and day of the week. I am not seeing a high degree of variability between days of the week. It looks like these folks are pretty consistent with their activity levels. Let's take a more zoomed in view of each activity type.


``` {r echo=TRUE}
very_active_df <-activity_duration_long_active %>%
            filter(activity_type == "very_active_minutes") 
       
ggplot(very_active_df,aes(activity_date_dow_desc,activity_minutes))+
      geom_violin(trim = FALSE)+
      stat_summary(fun.y=mean, geom="point", size=2, color="blue")+
      ylim(5,75)+
      
theme(legend.position = "bottom",axis.title.x = element_blank(),axis.title.y = element_blank())+
  labs(title = "Daily Minutes by Activity Type by Day of the Week",
              subtitle = "Very Active Minutes . = average daily minutes",
              caption = "Data source: Fitbit data set")

```
I have done a violin chart as it gives some idea of the distribution of the individual data points. The wider sections have more daily users for that number of minutes. The dot represents the average of all users for that day.

I was somewhat surprised at the average daily minutes by day. I would have assumed Sunday would have been higher as it is a weekend. As you can see, the average hovers around 30 minutes of very active minutes per day. I have limited the y range between 5 and 75 minutes per day. There were a few outliers that went as high as 120 minutes per day.

But overall, it looks like there is daily consistency for very active minutes. People are consistently are very active 30 minutes every day. This implies they have made a 30 minute workout part of their daily routine.

``` {r echo=TRUE}
fairly_active_df <-activity_duration_long_active %>%
            filter(activity_type == "fairly_active_minutes") 
       
ggplot(fairly_active_df,aes(activity_date_dow_desc,activity_minutes))+
      geom_violin(trim = FALSE)+
      stat_summary(fun.y=mean, geom="point", size=2, color="red")+
      ylim(5,75)+
      
theme(legend.position = "bottom",axis.title.x = element_blank(),axis.title.y = element_blank())+
  labs(title = "Daily Minutes by Activity Type by Day of the Week",
              subtitle = "Fairly Active Minutes . = average daily minutes",
              caption = "Data source: Fitbit data set")

```

This is showing the results of the fairly active minutes per day.  Monday - Thursday are very consistent at 20 or minutes per day. Sunday and Saturday are slightly higher.  Perhaps weekend errands accounting for the difference.

``` {r echo=TRUE}
lightly_active_df <-activity_duration_long_active %>%
            filter(activity_type == "lightly_active_minutes") 
       
ggplot(lightly_active_df,aes(activity_date_dow_desc,activity_minutes))+
      geom_violin(trim = FALSE)+
      stat_summary(fun.y=mean, geom="point", size=2, color="green")+
      ylim(5,300)+
      
  
      
theme(legend.position = "bottom",axis.title.x = element_blank(),axis.title.y = element_blank())+
  labs(title = "Daily Minutes by Activity Type by Day of the Week",
              subtitle = "Lightly Active Minutes . = average daily minutes",
              caption = "Data source: Fitbit data set")

```


The lightly active minutes is more or less the mirror image of the prior two plots, with the higher concentration of data points at the top.  The scale is significantly different because much more time is spent in this level of activity than the other two. I have explicitly excluded sedentary minutes because Bellabeat is fitness company and we want to understand how our users work out, and how they are spending their time.

We are again seeing consistency between days for this activity level throughout the week. The daily average is hovering around 190 minutes per day (3.2 hours per day).

## Share:

Part of the process is to share the analysis with other. I am sharing this analysis on my personal website which can be found here

[Personal Website](http://www.peterolesen.com)

I have created an markdown file in R showing the code as well as the steps of the analysis.

I have created a Github repository for this R project. It can be found here [Github Repository](https://github.com/pete-olesen/Google-Data-Analytics-Capstone-Bellabeat)

## Act:

**Executive Summary**

This is the executive summary for the Bellabeat:

Urska Srsen - Cofounder and Chief Creative Officer

Sando Mur - Cofounder

Bellabeat marketing analytics team

As you know, I was asked to evaluate some smart device data collected from Fitbit users. The objective being gain insight on trends in smart device usage, and could this information be used in Bellabeat's marketing strategy.

To that end, I obtained some publicly available data of Fitbit users. This data set is available on Kaggle and encompassed approximately 30 users over a two month time span in 2016.

I focused my analysis on the daily activity files. While there was data from both the minute and hour level of granularity, I wanted to review the daily results first, and see if you thought additional detailed analysis was warranted.

The daily data was summarized by user (id) by day and included activity minutes for four levels of activity intensity: very active, fairly active, lightly active, and sedentary.  There was also daily totals by user for distance for each of the activity levels as well as calories and steps taken. 

To keep the scope of the analysis manageable, I focused on the activity minutes for this phase (phase 1).  My assumption here is that there would likely be a strong correlation between minutes and distance as well as calories and steps taken. I am recommending a follow-up analysis, pending your review and approval.


**Conclusions**

Based on the analysis performed, the users demonstrate consistent minutes of activity by day of the week. This is true at all of the levels of intensity for the "active" activity types, specifically, very active, fairly active and lightly active.  I was expecting more variability with more activity on the weekends. This was not the case.

Here is a quick summary of average daily minutes of activity by activity type:

``` {r echo=TRUE}

  activity_summ <- activity_duration_long_active  %>% 
              select( activity_type, activity_minutes) %>% 
              tbl_summary(by = activity_type, statistic = (all_continuous() ~ "{mean} ({sd})")) %>% 
               modify_caption("**Table 1. Average Daily Minutes by Activity Type**") %>%
              add_overall()

  activity_summ
```
So as you can see, the average number of lightly active minutes is 193 with a standard deviation of 109 minutes, for example.

**Recommendations and comments**

I found this to be a useful exercise and data set to work with. However, a couple of observations:

>Data Limitations

* The data set was a limited number of users (approximately 30 users for 2 months of activity)
* There was no demographic data included in the data set (ie, gender, age, location, etc.)

I view this analysis as really a phase 1 exercise. I think it warrants a larger data set with more users as well as demographic data is warranted.  If the team here believes this is a worthwhile exercise, I am willing to search for additional data sets. Ideally, this would provide more data points as well as demographic data. Moreover, if you which I can do further analysis of this data set by minute/hour if you wish.

Please let me know how you wish to proceed. 

Thanks very much.

Pete Olesen







